{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "[ 3 49 65 94 84 40 54 78 53 89 98 16 74 50 58 31 43  4 75 44 83 84 13 66\n",
      " 15  6 73 22 73 31 36 27 94 88 12 28 21 25 20 60 84 65 69 58 23 76 18 52\n",
      " 54  9 48 47 64 81 83 36 58 21 81 20 62 88 34 92 79 82 20 32  4 84 36 35\n",
      " 72 60 71 72 52 50 54 11 51 18 47  5  8 37 97 20 33  1 59  1 56  1  9 57\n",
      " 20 79 29 16 32 54 93 10 46 59 84 76 15 10 15  0 69  4 51 51 94 36 39 62\n",
      "  2 24 26 35 25 87  0 55 34 38  1 45  7 93 56 38 21 51 75 81 74 33 20 37\n",
      "  9 40 60 31 83 50 71 67 30 66  1 43 61 23 65 84 87 46 57 16  2 28 12 96\n",
      " 44 76 29 75 41 87 67 61 30  5 12 62  3 83 81  6 85  4 37 57 84 39 71 61\n",
      "  6 76 14 31 98 40 17 51 16 42 63 86 37 69 86 71 80 78 14 35 25  5 39  8\n",
      "  9 26 44 60 13 14 77 13 80 87 18 60 78 92 51 45 78 41 51 30 14 35 46 21\n",
      "  8  6 92 38 40 15 32 17 93 71 92 27 78 15 19 60 21 38 36 49 74 67 95 31\n",
      " 82 45 16 83 63 80 42 22 74 53 15 44 47 57 94 76 17 32 24 15 93 24 80 59\n",
      " 46 12 51 77 79 70 69 16  2 63 83 55 12 53  1 67  0  2 36 42 10  9 52 59\n",
      "  6 22 86 31 51 37 43 75 90 24 86 96 45 32 98 36 66 48 73 73 79 56 41 21\n",
      " 25 27 97 18 44 45 40 80 63 20 35  0  8 27 25 35 59 61 21 37 29  6 19 78\n",
      " 50 54 37 93 33 46 79 59 29 43  0 23 17 38 66 38 89 17 25 31 65 10 26 86\n",
      " 58 42 46 24 95 93  8 53 32 14 10 94  8  8 64 44 74 30 97 22 11 68 56 90\n",
      " 96 16 43 57 91 24 28 82 90 64 61 92 28 84 70 45 85 34  7 88 89 61 26 88\n",
      " 41 46  8 91 41 14 98 28 26 36 70 74  7 52 70 42 66 22 13 44 91 53 22 16\n",
      " 40 40 28 70  6 60 95 23 16 50 29 49  9 18 55 63 60 19 28 30 31 85 66 88\n",
      " 63 83 64 96 13 34 27 95 36 72 29 91 22 65 71 66 11 32  2 75 39  5 37 67\n",
      " 81 55 61 57 81 82 63 55 54 35 86 25 24 96 10 58 59 28 89 54 52 85 68 69\n",
      "  8 39 95 39 82 48 74 52 74 55  9 47 84 91 12 96 82 64  7 40 73 77 11 36\n",
      " 68 23 28 46 75 43  2 11 47 53 56 62 62 80 56 30  3 88 37 33 73 76 21  5\n",
      " 76 87 68 83 62 57 47 19 88 96 42 23 44 87 82 49 63 24 94 69 54  5 79 43\n",
      " 12 50  5 52 92  4 84  1 33 49 26 18 44 13 24 73 89 78 67 41 11 46 47 69\n",
      "  0 18 98 44 85 29 53  1 45  3  9 13  2 66 59 79  6 17 43 83 26  1 12 49\n",
      " 71 89 58 93 39 42 15 38 55 15 93  4 90 88 55 40 55 17 34 94 57 92 81 26\n",
      " 60 89 49 89 30 65 58  4 19  4 76 74 71 21 54 13 16 72 68 62 61 25 72  7\n",
      " 12 18 77 90 62 14  3 78 65 37 27 50 95 98 60 72 58 38 87 93 19  7 83 50\n",
      "  3 91 77  7 64 61 69 23 76 65 48 41 92 20 91 18 70  9  9 29 85 67  0 35\n",
      " 98 91 90 31 53 39 24 85 96 17  7 11 96 39 56 90 79 45 64 97 41 19 74 11\n",
      " 10 62 95 28 96 10  7 68  7 93 34 42 68 41 14 22 58 12 71 27 98 72 91  3\n",
      " 43 19 61 75 20 81 63 67 56 26 47 11 31 57 62 66 19 75 97 94 13 75 95 32\n",
      " 50 97 52 87 32  3 47 77 48 33 73 64 49 68 43 94 77 68 47 82  2 30 23 33\n",
      " 34 66 33 35 88 68 27 87 54 79 34 67 65 18  4 26 30 52 86  0 29 80 67 95\n",
      " 39 25 70 58 35 27 17 38 91 13 23 77 79 77 22 49 98 48 46 48  5 63 97 80\n",
      " 53 20 25 78 10 65 33 41 85 90 98 97 71 95 52  3 29 69 51 70 27 22 34  6\n",
      " 48 72 21 89 17 97 72 80 10 57 64 92 38 15 73 87 73 48 42 82 33 56  3 42\n",
      "  1 53 55 90 19  6 30 86 64 49  2  8 45 76 92  0 23 69 59 80 90 32  5 59\n",
      " 85 89 94 45 48 86 81 14  4 77 56 82  2 85 70 88  0 75 14 86 81 97 70 72\n",
      " 34 40  5 11 78 50]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "\n",
    "def encode(train, test):\n",
    "    \"\"\"\n",
    "    处理训练集和测试集\n",
    "    \n",
    "    Arguments:\n",
    "    train -\n",
    "    test -\n",
    "    \n",
    "    Returns:\n",
    "    train - shape(990, 192) \n",
    "    labels - shape(900,) 数值\n",
    "    test - shape(594, 192)\n",
    "    classes - list 种类 len 99 \n",
    "    \"\"\"\n",
    "    label_encoder = LabelEncoder().fit(train.species)\n",
    "    labels = label_encoder.transform(train.species)\n",
    "    classes = list(label_encoder.classes_)\n",
    "\n",
    "    train = train.drop(['species', 'id'], axis=1)\n",
    "    test = test.drop('id', axis=1)\n",
    "\n",
    "    return train, labels, test, classes\n",
    "\n",
    "train, labels, test, classes = encode(train, test)\n",
    "\n",
    "# 标准化训练集\n",
    "scaler = StandardScaler().fit(train.values)\n",
    "scaled_train = scaler.transform(train.values)\n",
    "\n",
    "# 训练集和验证集分离\n",
    "sss = StratifiedShuffleSplit(test_size=0.1, random_state=23)\n",
    "for train_index, valid_index in sss.split(scaled_train, labels):\n",
    "    X_train, X_valid = scaled_train[train_index], scaled_train[valid_index]\n",
    "    y_train, y_valid = labels[train_index], labels[valid_index]\n",
    "    \n",
    "nb_features = 64 # number of features per features type (shape, texture, margin)   \n",
    "nb_class = len(classes)\n",
    "\n",
    "# reshape train data\n",
    "X_train_r = np.zeros((len(X_train), nb_features, 3))\n",
    "X_train_r[:, :, 0] = X_train[:, :nb_features]\n",
    "X_train_r[:, :, 1] = X_train[:, nb_features:128]\n",
    "X_train_r[:, :, 2] = X_train[:, 128:]\n",
    "\n",
    "# reshape validation data\n",
    "X_valid_r = np.zeros((len(X_valid), nb_features, 3))\n",
    "X_valid_r[:, :, 0] = X_valid[:, :nb_features]\n",
    "X_valid_r[:, :, 1] = X_valid[:, nb_features:128]\n",
    "X_valid_r[:, :, 2] = X_valid[:, 128:]\n",
    "\n",
    "# # Keras model with one Convolution1D layer\n",
    "# # unfortunately more number of covnolutional layers, filters and filters lenght \n",
    "# # don't give better accuracy\n",
    "# model = Sequential()\n",
    "# model.add(Convolution1D(nb_filter=512, filter_length=1, input_shape=(nb_features, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dropout(0.4))\n",
    "# model.add(Dense(2048, activation='relu'))\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dense(nb_class))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "# y_train = np_utils.to_categorical(y_train, nb_class)\n",
    "# y_valid = np_utils.to_categorical(y_valid, nb_class)\n",
    "\n",
    "# sgd = SGD(lr=0.01, nesterov=True, decay=1e-6, momentum=0.9)\n",
    "# model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n",
    "\n",
    "# nb_epoch = 15\n",
    "# model.fit(X_train_r, y_train, nb_epoch=nb_epoch, validation_data=(X_valid_r, y_valid), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
